{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSEUn2JBuCkc",
        "outputId": "3d0951d4-7d81-45c0-dc45-73092003212c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-2.6.1-py3-none-any.whl (163 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/163.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m153.6/163.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.3/163.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.38.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 sentence-transformers-2.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mP7L5N-_TfiG"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import json\n",
        "import pandas as pd\n",
        "import requests\n",
        "from transformers import pipeline\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ARa62i4sohr"
      },
      "outputs": [],
      "source": [
        "ENTITY_LINKING_API = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGG0Pe6MDFbR"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Py4MfQWiDTxa"
      },
      "outputs": [],
      "source": [
        "# %cd '/content/gdrive/MyDrive/similarity'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFIQvscxDl9M"
      },
      "source": [
        "Fine-tuning of the model on `data/extra_dataset`; this will result in a model that is saved inside `model/sentence_transformers`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WOVHl3Uus3K"
      },
      "outputs": [],
      "source": [
        "!python continue_training_models.py -model sentence-transformers/roberta-base-nli-mean-tokens -model_type sentence_bert -extra_dataset data/extra_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXAawtxLEptw"
      },
      "source": [
        "Transforming each \"sentence\"  in a corpus (`data/instructions_to_embed`) into an embedding representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENwN2DShVZSt"
      },
      "source": [
        "Load in test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhDFU3t5VIVn",
        "outputId": "bc4b14b1-557a-4cb8-fa9e-f031b0258e94"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'inp_instruction': 'Easy chicken fajitas: Heat a griddle pan until smoking hot and\\xa0add the chicken and marinade to the pan.',\n",
              "  'inp_ingred_full_name': 'chicken breasts',\n",
              "  'inp_ingred_matched_name': 'Chicken:Breast',\n",
              "  'ref_ingred_matched_name': 'Tofu:Firm'},\n",
              " {'inp_instruction': 'Moroccan Pastilla: Take a sheet of filo pastry and brush it with melted butter.',\n",
              "  'inp_ingred_full_name': 'butter',\n",
              "  'inp_ingred_matched_name': 'Butter',\n",
              "  'ref_ingred_matched_name': 'Olive oil'},\n",
              " {'inp_instruction': 'Blended Vegetable Soup: In a large pot over high heat, add extra-virgin oil, garlic, ginger and red onions.',\n",
              "  'inp_ingred_full_name': 'extra virgin olive oil',\n",
              "  'inp_ingred_matched_name': 'Olive oil',\n",
              "  'ref_ingred_matched_name': 'Sunflower oil'},\n",
              " {'inp_instruction': 'Broccoli pesto pasta: Drain the pasta and return it to the pan.',\n",
              "  'inp_ingred_full_name': 'pasta',\n",
              "  'inp_ingred_matched_name': 'Pasta',\n",
              "  'ref_ingred_matched_name': 'White rice'},\n",
              " {'inp_instruction': 'Vegetarian tacos: For the beans, heat a frying pan over a medium heat.',\n",
              "  'inp_ingred_full_name': 'black beans',\n",
              "  'inp_ingred_matched_name': 'Black bean',\n",
              "  'ref_ingred_matched_name': 'Kidney bean'},\n",
              " {'inp_instruction': 'My Favorite Vegan Pizza: Note: If using tomato paste, add water to thin until desired consistency is reached.',\n",
              "  'inp_ingred_full_name': 'tomato sauce',\n",
              "  'inp_ingred_matched_name': 'Tomato sauce',\n",
              "  'ref_ingred_matched_name': 'Salsa'},\n",
              " {'inp_instruction': 'The Best Vegan Burgers: Add in the remaining black beans and pulse until combined, still leaving it quite chunky so you can partially see the black beans',\n",
              "  'inp_ingred_full_name': 'black beans',\n",
              "  'inp_ingred_matched_name': 'Black bean',\n",
              "  'ref_ingred_matched_name': 'Kidney bean'},\n",
              " {'inp_instruction': 'Lentil Bolognese: Stir in tomato paste, and cook, stirring it in, for 1 minutes.',\n",
              "  'inp_ingred_full_name': 'tomato sauce',\n",
              "  'inp_ingred_matched_name': 'Tomato sauce',\n",
              "  'ref_ingred_matched_name': 'Tomato puree'},\n",
              " {'inp_instruction': 'Crispy Sesame Chicken with a Sticky Asian Sauce: Add the chicken back in and toss in the sauce to coat.',\n",
              "  'inp_ingred_full_name': 'chicken breast fillets',\n",
              "  'inp_ingred_matched_name': 'Chicken:Fillet',\n",
              "  'ref_ingred_matched_name': 'Quorn sausage'},\n",
              " {'inp_instruction': 'Homemade Vegetarian Chili: Add the diced tomatoes and their juices, the drained black beans and pinto beans, vegetable broth and bay leaf.',\n",
              "  'inp_ingred_full_name': 'black beans',\n",
              "  'inp_ingred_matched_name': 'Black bean',\n",
              "  'ref_ingred_matched_name': 'Cannellini bean'}]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_data = []\n",
        "\n",
        "with open('data/extra_dataset/test.csv', 'r') as csvfile:\n",
        "  csvdictreader = csv.DictReader(csvfile)\n",
        "  for row in csvdictreader:\n",
        "    test_sample = {\n",
        "        'inp_instruction': row['anchor'],\n",
        "        'inp_ingred_full_name': row['anchor_full_ingred_name'],\n",
        "        'inp_ingred_matched_name': row['anchor_matched_ingred_name'],\n",
        "        'ref_ingred_matched_name': row['pos_matched_ingred_name']\n",
        "    }\n",
        "    test_data.append(test_sample)\n",
        "\n",
        "test_data[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4DIz9empuLS"
      },
      "source": [
        "Load NER model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MW6OuD5Pptp0"
      },
      "outputs": [],
      "source": [
        "ner_model_path = 'food_ner'\n",
        "ner = pipeline('ner', model=ner_model_path + '/bert/bert', ignore_labels=[])\n",
        "\n",
        "#load labels json\n",
        "labels_filename = ner_model_path + '/labels.json'\n",
        "json_file = open(labels_filename, 'r')\n",
        "json_object = json.loads(json_file.read())\n",
        "json_file.close()\n",
        "\n",
        "original_labels_dict = json_object['labels']\n",
        "\n",
        "# load config file\n",
        "config_filename = ner_model_path + '/bert/bert/config.json'\n",
        "json_file = open(config_filename, 'r')\n",
        "json_object = json.loads(json_file.read())\n",
        "json_file.close()\n",
        "\n",
        "label_dict = json_object['label2id']\n",
        "\n",
        "def get_nes(ner_results, input_text):\n",
        "  ne_strings = []\n",
        "  named_entities = []\n",
        "  current_ne = None\n",
        "  ne_end = 0\n",
        "  for result in ner_results:\n",
        "    entity_label = result['entity']\n",
        "    token = result['word']\n",
        "    score = result['score']\n",
        "    label_str = str(label_dict[entity_label])\n",
        "    label = original_labels_dict[label_str]\n",
        "    start_char = result['start']\n",
        "    end_char = result['end']\n",
        "    #print(token , label, score, start_char, end_char)\n",
        "    if 'B-FOOD' in label:\n",
        "      if current_ne == None:\n",
        "        current_ne = (start_char, end_char, score)\n",
        "      elif '##' in token:\n",
        "        #named_entities.append(current_ne)\n",
        "        current_ne = (current_ne[0], end_char, score)\n",
        "    elif label == 'O':\n",
        "      if '##' in token and current_ne!=None:\n",
        "        current_ne = (current_ne[0], end_char, score)\n",
        "      elif current_ne != None:\n",
        "        named_entities.append(current_ne)\n",
        "        current_ne = None\n",
        "\n",
        "    elif 'I-FOOD' in label:\n",
        "      if current_ne != None:\n",
        "        current_ne = (current_ne[0], end_char, score)\n",
        "      else:\n",
        "        current_ne = (start_char, end_char, score)\n",
        "  if current_ne != None:\n",
        "    named_entities.append(current_ne)\n",
        "\n",
        "  for ne in named_entities:\n",
        "    ne_strings.append(input_text[ne[0]:ne[1]])\n",
        "\n",
        "  return ne_strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3mMjhPSus8ng"
      },
      "outputs": [],
      "source": [
        "def get_normalised_food(url, ingredient):\n",
        "    url = f'{url}{ingredient}'\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "\n",
        "        # Check if the request was successful (status code 200)\n",
        "        if response.status_code == 200:\n",
        "            # If successful, return the response content and decode from bytes to string\n",
        "            return response.content.decode('UTF-8')\n",
        "        else:\n",
        "            # If not successful, print an error message\n",
        "            print(f\"Failed to fetch data. Status code: {response.status_code}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        # Print any exception that occurred during the request\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KiLDf2KqwhwH"
      },
      "outputs": [],
      "source": [
        "def place_all_ingreds_in_instruction(instruction):\n",
        "    # Read all ingredients\n",
        "    ingreds = set()\n",
        "    with open('data/extra_dataset/KB.json', 'r', encoding='utf-8') as file:\n",
        "    # Load JSON data from the file\n",
        "        data = json.load(file)\n",
        "        for ingred in data:\n",
        "            ingreds.add(ingred['ingredient'])\n",
        "\n",
        "    # Save all instruction versions to .tsv\n",
        "    with open('data/instructions_to_embed/input.tsv', 'w', newline='', encoding='utf-8') as tsv_file:\n",
        "        writer = csv.writer(tsv_file, delimiter='\\t')\n",
        "        for ingred in ingreds:\n",
        "            writer.writerow([instruction.replace('{0}', ingred.lower())])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KF80YGRGE-0o"
      },
      "source": [
        "Given a query, find the top 5 \"sentences\" in the corpus that are most similar to the query, based on their embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSl5aXlJWL0z",
        "outputId": "d076579f-d1bb-41d5-ab82-bc2f60e6ef74"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/587 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------\n",
            "['Easy chicken fajitas: Heat a griddle pan until smoking hot and\\xa0add the buckwheat flour and marinade to the pan.', 'Easy chicken fajitas: Heat a griddle pan until smoking hot and\\xa0add the shrimp and marinade to the pan.', 'Easy chicken fajitas: Heat a griddle pan until smoking hot and\\xa0add the brawn and marinade to the pan.', 'Easy chicken fajitas: Heat a griddle pan until smoking hot and\\xa0add the blackcurrant and marinade to the pan.', 'Easy chicken fajitas: Heat a griddle pan until smoking hot and\\xa0add the pomfret and marinade to the pan.', 'Easy chicken fajitas: Heat a griddle pan until smoking hot and\\xa0add the kippers and marinade to the pan.', 'Easy chicken fajitas: Heat a griddle pan until smoking hot and\\xa0add the advocaat and marinade to the pan.', 'Easy chicken fajitas: Heat a griddle pan until smoking hot and\\xa0add the sausage and marinade to the pan.', 'Easy chicken fajitas: Heat a griddle pan until smoking hot and\\xa0add the rabbit and marinade to the pan.', 'Easy chicken fajitas: Heat a griddle pan until smoking hot and\\xa0add the mirin and marinade to the pan.']\n",
            "2024-04-01 14:40:12.354169: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-01 14:40:12.354237: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-01 14:40:12.355646: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-01 14:40:13.755601: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Loading NER model...\n",
            "['chicken', 'marinade']\n",
            "chicken\n",
            "Replace detected raw string ingredient in query: chicken\n",
            "Query normalised raw string ingredient: Chicken\n",
            "Query Co2: 6.092\n",
            "['chicken']\n",
            "Removing suggestion with same name: Chicken\n",
            "['turkey']\n",
            "Removing suggestion with >= Co2: Turkey, 6.092\n",
            "['chicken stock']\n",
            "['pork']\n",
            "Removing suggestion with >= Co2: Pork, 7.282\n",
            "['tofu']\n",
            "['quorn']\n",
            "['dab']\n",
            "['olive']\n",
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "Query: Easy chicken fajitas: Heat a griddle pan until smoking hot and add the chicken and marinade to the pan.\n",
            "\n",
            "Top 5 most similar sentences in corpus:\n",
            "Easy chicken fajitas: Heat a griddle pan until smoking hot and add the chicken stock and marinade to the pan. | Score: 0.9588 | Co2: 0.3\n",
            "Easy chicken fajitas: Heat a griddle pan until smoking hot and add the tofu and marinade to the pan. | Score: 0.9331 | Co2: 2.871\n",
            "Easy chicken fajitas: Heat a griddle pan until smoking hot and add the quorn and marinade to the pan. | Score: 0.9316 | Co2: 2.555\n",
            "Easy chicken fajitas: Heat a griddle pan until smoking hot and add the dab and marinade to the pan. | Score: 0.9086 | Co2: 2.827\n",
            "Easy chicken fajitas: Heat a griddle pan until smoking hot and add the olive and marinade to the pan. | Score: 0.9041 | Co2: 0.455\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 1/587 [00:48<7:56:11, 48.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------\n",
            "['Moroccan Pastilla: Take a sheet of filo pastry and brush it with buckwheat flour.', 'Moroccan Pastilla: Take a sheet of filo pastry and brush it with shrimp.', 'Moroccan Pastilla: Take a sheet of filo pastry and brush it with brawn.', 'Moroccan Pastilla: Take a sheet of filo pastry and brush it with blackcurrant.', 'Moroccan Pastilla: Take a sheet of filo pastry and brush it with pomfret.', 'Moroccan Pastilla: Take a sheet of filo pastry and brush it with kippers.', 'Moroccan Pastilla: Take a sheet of filo pastry and brush it with advocaat.', 'Moroccan Pastilla: Take a sheet of filo pastry and brush it with sausage.', 'Moroccan Pastilla: Take a sheet of filo pastry and brush it with rabbit.', 'Moroccan Pastilla: Take a sheet of filo pastry and brush it with mirin.']\n",
            "2024-04-01 14:40:56.003786: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-01 14:40:56.003870: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-01 14:40:56.005273: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-01 14:40:57.406754: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Loading NER model...\n",
            "['filo pastry', 'melted butter']\n",
            "melted butter\n",
            "Replace detected raw string ingredient in query: melted butter\n",
            "Query normalised raw string ingredient: Butter\n",
            "Query Co2: 2.781\n",
            "['butter']\n",
            "Removing suggestion with same name: Butter\n",
            "['olive oil']\n",
            "Removing suggestion with >= Co2: Olive oil, 5.944\n",
            "['vegetable oil']\n",
            "Removing suggestion with >= Co2: Vegetable oil, 3.828\n",
            "['oil']\n",
            "Removing suggestion with >= Co2: Oil, 3.828\n",
            "['margarine']\n",
            "Removing suggestion with >= Co2: Margarine, 3.828\n",
            "['coconut oil']\n",
            "Removing suggestion with >= Co2: Coconut oil, 5.752\n",
            "['pastry']\n",
            "Removing suggestion with >= Co2: Pastry, 7.605\n",
            "['sunflower oil']\n",
            "Removing suggestion with >= Co2: Sunflower oil, 3.607\n",
            "['dough']\n",
            "['mixed herbs']\n",
            "['olive']\n",
            "[]\n",
            "Removing suggestion already in instruction: Olive\n",
            "['mixed peel']\n",
            "['garlic']\n",
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "Query: Moroccan Pastilla: Take a sheet of filo pastry and brush it with melted butter.\n",
            "\n",
            "Top 5 most similar sentences in corpus:\n",
            "Moroccan Pastilla: Take a sheet of filo pastry and brush it with dough. | Score: 0.9607 | Co2: 1.441\n",
            "Moroccan Pastilla: Take a sheet of filo pastry and brush it with mixed herbs. | Score: 0.96 | Co2: 0.455\n",
            "Moroccan Pastilla: Take a sheet of filo pastry and brush it with olive. | Score: 0.9599 | Co2: 0.455\n",
            "Moroccan Pastilla: Take a sheet of filo pastry and brush it with mixed peel. | Score: 0.9557 | Co2: 0.317\n",
            "Moroccan Pastilla: Take a sheet of filo pastry and brush it with garlic. | Score: 0.9549 | Co2: 0.395\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 2/587 [01:35<7:41:51, 47.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nope\n",
            "-----------------\n",
            "[\"'Moroccan Pastilla: Take a sheet of filo pastry and brush it with melted butter.'\", \"'Moroccan Pastilla: Take a sheet of filo pastry and brush it with melted butter.'\", \"'Moroccan Pastilla: Take a sheet of filo pastry and brush it with melted butter.'\", \"'Moroccan Pastilla: Take a sheet of filo pastry and brush it with melted butter.'\", \"'Moroccan Pastilla: Take a sheet of filo pastry and brush it with melted butter.'\", \"'Moroccan Pastilla: Take a sheet of filo pastry and brush it with melted butter.'\", \"'Moroccan Pastilla: Take a sheet of filo pastry and brush it with melted butter.'\", \"'Moroccan Pastilla: Take a sheet of filo pastry and brush it with melted butter.'\", \"'Moroccan Pastilla: Take a sheet of filo pastry and brush it with melted butter.'\", \"'Moroccan Pastilla: Take a sheet of filo pastry and brush it with melted butter.'\"]\n",
            "2024-04-01 14:41:42.659352: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-01 14:41:42.659418: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-01 14:41:42.660807: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-01 14:41:44.072807: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Loading NER model...\n",
            "['extra', 'virgin oil', 'garlic', 'ginger', 'red onions']\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/gdrive/MyDrive/similarity/text_search.py\", line 270, in <module>\n",
            "    main(model_path, model_type, corpus_embedding, query, query_ingredient, ref_ingredient)\n",
            "  File \"/content/gdrive/MyDrive/similarity/text_search.py\", line 197, in main\n",
            "    top_results = get_top_k_results(sentences, query, query_ingredient, results, 5)\n",
            "  File \"/content/gdrive/MyDrive/similarity/text_search.py\", line 123, in get_top_k_results\n",
            "    query_nes.remove(query_ingredient)\n",
            "ValueError: list.remove(x): x not in list\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  1%|          | 3/587 [02:12<6:55:23, 42.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------\n",
            "['Broccoli pesto pasta: Drain the buckwheat flour and return it to the pan.', 'Broccoli pesto pasta: Drain the shrimp and return it to the pan.', 'Broccoli pesto pasta: Drain the brawn and return it to the pan.', 'Broccoli pesto pasta: Drain the blackcurrant and return it to the pan.', 'Broccoli pesto pasta: Drain the pomfret and return it to the pan.', 'Broccoli pesto pasta: Drain the kippers and return it to the pan.', 'Broccoli pesto pasta: Drain the advocaat and return it to the pan.', 'Broccoli pesto pasta: Drain the sausage and return it to the pan.', 'Broccoli pesto pasta: Drain the rabbit and return it to the pan.', 'Broccoli pesto pasta: Drain the mirin and return it to the pan.']\n",
            "2024-04-01 14:42:16.577435: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-01 14:42:16.577501: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-01 14:42:16.578846: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-01 14:42:17.961609: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Loading NER model...\n",
            "['pasta']\n",
            "pasta\n",
            "Replace detected raw string ingredient in query: pasta\n",
            "Query normalised raw string ingredient: Pasta\n",
            "Query Co2: 1.441\n",
            "['pasta']\n",
            "Removing suggestion with same name: Pasta\n",
            "['spaghetti']\n",
            "Removing suggestion with >= Co2: Spaghetti, 1.441\n",
            "['passata']\n",
            "['mascarpone']\n",
            "Removing suggestion with >= Co2: Mascarpone, 21.24\n",
            "['pesto']\n",
            "['dripping']\n",
            "Removing suggestion with >= Co2: Dripping, 59.57\n",
            "['dab']\n",
            "Removing suggestion with >= Co2: Dab, 2.827\n",
            "['risotto rice']\n",
            "Removing suggestion with >= Co2: Risotto rice, 3.839\n",
            "['ortaniques']\n",
            "['paella rice']\n",
            "Removing suggestion with >= Co2: Paella rice, 3.839\n",
            "['miso']\n",
            "Removing suggestion with >= Co2: Miso, 2.871\n",
            "['halloumi']\n",
            "Removing suggestion with >= Co2: Halloumi, 21.24\n",
            "['rice']\n",
            "Removing suggestion with >= Co2: Rice, 3.839\n",
            "['spaghetti squash']\n",
            "['pastrami']\n",
            "Removing suggestion with >= Co2: Pastrami, 59.57\n",
            "['courgette']\n",
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "Query: Broccoli pesto pasta: Drain the pasta and return it to the pan.\n",
            "\n",
            "Top 5 most similar sentences in corpus:\n",
            "Broccoli pesto pasta: Drain the passata and return it to the pan. | Score: 0.9535 | Co2: 1.427\n",
            "Broccoli pesto pasta: Drain the pesto and return it to the pan. | Score: 0.9448 | Co2: 0.437\n",
            "Broccoli pesto pasta: Drain the ortaniques and return it to the pan. | Score: 0.9318 | Co2: 0.317\n",
            "Broccoli pesto pasta: Drain the spaghetti squash and return it to the pan. | Score: 0.9224 | Co2: 0.455\n",
            "Broccoli pesto pasta: Drain the courgette and return it to the pan. | Score: 0.9161 | Co2: 0.455\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  1%|          | 4/587 [02:57<7:05:14, 43.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------\n",
            "['Vegetarian tacos: For the buckwheat flour, heat a frying pan over a medium heat.', 'Vegetarian tacos: For the shrimp, heat a frying pan over a medium heat.', 'Vegetarian tacos: For the brawn, heat a frying pan over a medium heat.', 'Vegetarian tacos: For the blackcurrant, heat a frying pan over a medium heat.', 'Vegetarian tacos: For the pomfret, heat a frying pan over a medium heat.', 'Vegetarian tacos: For the kippers, heat a frying pan over a medium heat.', 'Vegetarian tacos: For the advocaat, heat a frying pan over a medium heat.', 'Vegetarian tacos: For the sausage, heat a frying pan over a medium heat.', 'Vegetarian tacos: For the rabbit, heat a frying pan over a medium heat.', 'Vegetarian tacos: For the mirin, heat a frying pan over a medium heat.']\n",
            "2024-04-01 14:43:03.847182: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-01 14:43:03.847256: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-01 14:43:03.848626: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-01 14:43:05.313578: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Loading NER model...\n",
            "['beans']\n",
            "beans\n",
            "Replace detected raw string ingredient in query: beans\n",
            "Query normalised raw string ingredient: Bean\n",
            "Query Co2: 1.61\n",
            "['quorn']\n",
            "Removing suggestion with >= Co2: Quorn, 2.555\n",
            "['tofu']\n",
            "Removing suggestion with >= Co2: Tofu, 2.871\n",
            "['kidney']\n",
            "Removing suggestion with >= Co2: Kidney, 24.415\n",
            "['turkey']\n",
            "Removing suggestion with >= Co2: Turkey, 6.092\n",
            "['chicken']\n",
            "Removing suggestion with >= Co2: Chicken, 6.092\n",
            "[]\n",
            "Removing suggestion already in instruction: Chicken\n",
            "[]\n",
            "Removing suggestion already in instruction: Chicken\n",
            "['lentil']\n",
            "Removing suggestion with >= Co2: Lentil, 1.61\n",
            "['ling']\n",
            "Removing suggestion with >= Co2: Ling, 2.827\n",
            "['pork']\n",
            "Removing suggestion with >= Co2: Pork, 7.282\n",
            "['quinoa']\n",
            "['ortaniques']\n",
            "['seitan']\n",
            "['beef']\n",
            "Removing suggestion with >= Co2: Beef, 59.57\n",
            "[]\n",
            "Removing suggestion already in instruction: Beef\n",
            "['halloumi']\n",
            "Removing suggestion with >= Co2: Halloumi, 21.24\n",
            "['courgette']\n",
            "['salsa']\n",
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "Query: Vegetarian tacos: For the beans, heat a frying pan over a medium heat.\n",
            "\n",
            "Top 5 most similar sentences in corpus:\n",
            "Vegetarian tacos: For the quinoa, heat a frying pan over a medium heat. | Score: 0.9473 | Co2: 1.441\n",
            "Vegetarian tacos: For the ortaniques, heat a frying pan over a medium heat. | Score: 0.9439 | Co2: 0.317\n",
            "Vegetarian tacos: For the seitan, heat a frying pan over a medium heat. | Score: 0.9439 | Co2: 1.441\n",
            "Vegetarian tacos: For the courgette, heat a frying pan over a medium heat. | Score: 0.9387 | Co2: 0.455\n",
            "Vegetarian tacos: For the salsa, heat a frying pan over a medium heat. | Score: 0.9374 | Co2: 1.427\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  1%|          | 5/587 [03:45<7:17:17, 45.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nope\n",
            "-----------------\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_common.py\", line 51, in <module>\n",
            "    from ._text_generation import TextGenerationStreamResponse, _parse_text_generation_error\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_text_generation.py\", line 35, in <module>\n",
            "    if is_pydantic_available():\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_runtime.py\", line 181, in is_pydantic_available\n",
            "    from pydantic import validator  # noqa: F401\n",
            "  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/__init__.py\", line 383, in __getattr__\n",
            "    module = import_module(module_name, package=package)\n",
            "  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/deprecated/class_validators.py\", line 12, in <module>\n",
            "    from .._internal import _decorators, _decorators_v1\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_decorators.py\", line 11, in <module>\n",
            "    from pydantic_core import PydanticUndefined, core_schema\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic_core/__init__.py\", line 30, in <module>\n",
            "    from .core_schema import CoreConfig, CoreSchema, CoreSchemaType, ErrorType\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic_core/core_schema.py\", line 2010, in <module>\n",
            "    class NoInfoWrapValidatorFunctionSchema(TypedDict):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/typing_extensions.py\", line 910, in __new__\n",
            "    own_annotations = {\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/typing_extensions.py\", line 911, in <dictcomp>\n",
            "    n: typing._type_check(tp, msg, module=tp_dict.__module__)\n",
            "  File \"/usr/lib/python3.10/typing.py\", line 164, in _type_check\n",
            "    arg = _type_convert(arg, module=module, allow_special_forms=allow_special_forms)\n",
            "  File \"/usr/lib/python3.10/typing.py\", line 142, in _type_convert\n",
            "    return ForwardRef(arg, module=module, is_class=allow_special_forms)\n",
            "  File \"/usr/lib/python3.10/typing.py\", line 668, in __init__\n",
            "    code = compile(arg, '<string>', 'eval')\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/gdrive/MyDrive/similarity/process_sentence_corpus.py\", line 1, in <module>\n",
            "    from sentence_transformers import SentenceTransformer, models\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sentence_transformers/__init__.py\", line 3, in <module>\n",
            "    from .datasets import SentencesDataset, ParallelSentencesDataset\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sentence_transformers/datasets/__init__.py\", line 1, in <module>\n",
            "    from .DenoisingAutoEncoderDataset import DenoisingAutoEncoderDataset\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sentence_transformers/datasets/DenoisingAutoEncoderDataset.py\", line 5, in <module>\n",
            "    from transformers.utils.import_utils import is_nltk_available, NLTK_IMPORT_ERROR\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/__init__.py\", line 26, in <module>\n",
            "    from . import dependency_versions_check\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
            "    from .utils.versions import require_version, require_version_core\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/__init__.py\", line 18, in <module>\n",
            "    from huggingface_hub import get_full_repo_name  # for backward compatibility\n",
            "  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/__init__.py\", line 370, in __getattr__\n",
            "    submod = importlib.import_module(submod_path)\n",
            "  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/hf_api.py\", line 62, in <module>\n",
            "    from ._inference_endpoints import InferenceEndpoint, InferenceEndpointType\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/_inference_endpoints.py\", line 7, in <module>\n",
            "    from .inference._client import InferenceClient\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py\", line 56, in <module>\n",
            "    from huggingface_hub.inference._common import (\n",
            "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 694, in _load_unlocked\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/fx/passes/reinplace.py\", line 4, in <module>\n",
            "    from torch._subclasses.fake_tensor import FakeTensorMode, FakeTensor\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/__init__.py\", line 3, in <module>\n",
            "    from torch._subclasses.fake_tensor import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py\", line 702, in <module>\n",
            "    @register_op_impl(aten.slice_scatter.out)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_ops.py\", line 733, in __getattr__\n",
            "    op_, op_dk_, tags = torch._C._get_operation_overload(\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/gdrive/MyDrive/similarity/text_search.py\", line 9, in <module>\n",
            "    from sentence_transformers import SentenceTransformer, models\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sentence_transformers/__init__.py\", line 3, in <module>\n",
            "    from .datasets import SentencesDataset, ParallelSentencesDataset\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sentence_transformers/datasets/__init__.py\", line 1, in <module>\n",
            "    from .DenoisingAutoEncoderDataset import DenoisingAutoEncoderDataset\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sentence_transformers/datasets/DenoisingAutoEncoderDataset.py\", line 1, in <module>\n",
            "    from torch.utils.data import Dataset\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 1829, in <module>\n",
            "    from torch import export as export\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/export/__init__.py\", line 29, in <module>\n",
            "    from torch.fx.passes.infra.pass_base import PassResult\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/fx/passes/__init__.py\", line 6, in <module>\n",
            "    from . import reinplace\n",
            "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  1%|          | 6/587 [03:50<5:04:59, 31.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n",
            "-----------------\n"
          ]
        }
      ],
      "source": [
        "for sample in tqdm(test_data):\n",
        "\n",
        "  # Extract instruction and identify all food items\n",
        "  instruction_without_recipe_name = sample['inp_instruction'][sample['inp_instruction'].find(':') + 1:]\n",
        "  ner_results = ner(instruction_without_recipe_name)\n",
        "  nes = get_nes(ner_results, instruction_without_recipe_name)\n",
        "\n",
        "  # Identify food item closest to targeted item to be replced\n",
        "  raw_string_to_replace = ''\n",
        "\n",
        "  if(len(nes) == 1):\n",
        "    raw_string_to_replace = nes[0]\n",
        "  if(len(nes) > 1):\n",
        "    for raw_entity in nes:\n",
        "      normalised = get_normalised_food(ENTITY_LINKING_API, raw_entity)\n",
        "      if(normalised == sample['inp_ingred_matched_name'] or\n",
        "         normalised in sample['inp_ingred_matched_name'] or\n",
        "         sample['inp_ingred_matched_name'] in normalised):\n",
        "        raw_string_to_replace = raw_entity\n",
        "        break\n",
        "\n",
        "  # Replace ingredient with {0}\n",
        "  if(raw_string_to_replace):\n",
        "    instruction_text = instruction_without_recipe_name.replace(raw_string_to_replace, '{0}')\n",
        "    query = sample['inp_instruction'][:sample['inp_instruction'].find(':') + 1] + instruction_text\n",
        "  else:\n",
        "    print('nope')\n",
        "  print('-----------------')\n",
        "\n",
        "  # Replace {0} with knowledge base foods to make all sentence suggestions\n",
        "  place_all_ingreds_in_instruction(query)\n",
        "\n",
        "  # Compute embeddings\n",
        "  !python process_sentence_corpus.py -model model_iterations/2/roberta-base-nli-mean-tokens_continue_training_2024_03_03_13_27_15 -model_type sentence_bert -sentences data/instructions_to_embed -output data/output/\n",
        "\n",
        "  # Filter and rank best suggestions\n",
        "  query = r\"'%s'\"%(sample['inp_instruction'])\n",
        "  query_ingredient =  r\"'%s'\"%(raw_string_to_replace)\n",
        "  ref_ingredient =  r\"'%s'\"%(sample[\"ref_ingred_matched_name\"])\n",
        "\n",
        "  !python text_search.py -model model_iterations/2/roberta-base-nli-mean-tokens_continue_training_2024_03_03_13_27_15 -model_type sentence_bert -embeddings data/output/ -query {query} -query_ingredient {query_ingredient} -ref_ingredient {ref_ingredient}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPz1AGIe1ybd"
      },
      "outputs": [],
      "source": [
        "# !python text_search.py -model model_iterations/2/roberta-base-nli-mean-tokens_continue_training_2024_03_03_13_27_15 -model_type sentence_bert -embeddings data/output/ -query {query}"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
